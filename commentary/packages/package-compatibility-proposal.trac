= Package Compatibility =

In GHC 6.8.1 we reorganised some of the contents of the packages we ship with GHC, see #710.  The idea was to lessen the problem caused by the base package being essentially static between GHC major releases.  By separating modules out of base and putting them into separate packages, it is possible to updgrade these modules independently of GHC.

The reorganisations unfortunately exposed some problems with our package infrastructure, in particular most packages that compiled with 6.6 do not compile with 6.8.1 because they don't depend on the new packages.  We anticipated the problem to some extent, adding "configurations" to Cabal to make it possible to write conditional package specifications that work with multiple sets of dependencies.  We are still left with the problem that the `.cabal` files for all packages need to be updated for GHC 6.8.1.  This seems like the wrong way around: the change we made to a few packages has to be propagated everywhere, when there should be a way to confine it locally, at least for the purposes of continued compatibility with existing source code.  In many cases, the underlying APIs are still available, just from a different place.  (in general this may not be true - modifications to packages may make changes to APIs which require real changes to dependent packages).

Some of the problems that contributed to this situation can be addressed.  We wrote the [http://haskell.org/haskellwiki/Package_versioning_policy Package Versioning Policy] so that packages can start using versions that reflect API changes, and so that dependencies can start being precise about which dependencies they work with. If we follow these guidelines, then 

 * failures will be more predictable
 * failures will be more informative

because dependencies and API changes are better documented.  However, we have no fewer failures than before, in fact we have more because packages cannot now "accidentally work" by specifying loose dependency ranges.

So the big question is, what changes do we need to make in the future to either prevent this happening, or to reduce the pain when it does happen?  Below are collected various proposals.  If the proposals get too long we can separate them out into new pages.

== 1. Don't reorganise packages ==

We could do this, but that just hides the problem and we're still left with a monolithic base package.  We still have to contend with API changes causing breakage.

== 2. Provide older version(s) of base with a new GHC release ==

We could fork the base package for each new release, and keep compiling the old one(s).  Unfortunately we would then have to compile every other package two (or more) times, once against each version of base.  And if we were to give the same treatment to any other library, we end up with exponential blowup in the number of copies.

The GHC build gets slower, and the testing surface increases for each release.

Furthermore, the package database cannot currently understand multiple packages compiled against different versions of dependencies.  One workaround is to have multiple package databases, but that's not too convenient.

== 3. Provide older versions of base that re-export the new version ==

To do..

== 4. Do some kind of provides/requires interface in Cabal ==

To do..

