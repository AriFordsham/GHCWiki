= Proposed Change to GHC Performance Tests

GHC has a test suite that can be run via `make test` (see [https://ghc.haskell.org/trac/ghc/wiki/Building/RunningTests/Running Running the Testsuite]). A subset of the tests are performance tests, which either test the performance of code generated by GHC or test the performance of GHC itself. This proposal is concerned with such performance tests and is motivated by various shortcomings in the current system. The fundamental changes proposed here are:
* No longer specify expected performance results in `*.T` files.
  * Performance tests run against the previous commit's results instead of the expected results.
* Log performance results (using [https://git-scm.com/docs/git-notes git notes]) for all test runs.
  * The CI server will automatically record performance results for each merged commit, pushing the performance results to the git.haskell.com repo.
  * A simple CLI tool is provided along side the test driver to analyse previous results.
* Recording and comparing performance results is always done with respect to a [#Platform platform].

[=#CurrentSystem]
== Current System

* Tests are listed in `.T` files along with manually chosen expected value and tolerance percentage. Expected values may be, but often aren't, platform specific. e.g.  testing 'bytes allocated' with a tolerance of 10%:
{{{
test('T5631',
  [
    compiler_stats_num_field(
      'bytes allocated',
      [
        (wordsize(32), 570137436, 10),
        (wordsize(64), 1106015512, 5)
      ]),
    only_ways(['normal'])
  ],
  compile,
  [''])
}}}
* When running performance tests:
  * The results are compared to the expected values with respective tolerances.
  * Tests fail if they are outside the tolerance percentage from the expected value.

=== Issues with the current system

* **The programmer must specify expected values and tolerances.** This seems like a reasonable expectation but is a source of frustration in practice. The CI server will run tests on various platforms which may all give varying results, making it hard for the programmer to pick an appropriate value. This leads to a time consuming workflow of pushing code to the CI server and manually observing its output to derive expected values. This also leads to other issues:
  * **The programmer must update expected values.** When performance significantly changes, such that performance tests fail, the programmer must pick new expected values and/or adjust the tolerance values. Picking these values again is more effort than we'd like.
  * **Expected values are usually NOT per platform.** While it is currently possible to give expected values per platform, doing so takes extra effort for the programmer and is often not done. Instead the programmer often picks some middle value with a tolerance that works for all platforms.
  * **Tolerance intervals are too large.** As the expected values are usually not platform specific, the tolerance values must be larger to make the tests pass in all cases (avoiding false positives). This leads to more false negatives (tests pass even though performance has changed unexpectedly).
* **Performance test results are not logged.** Currently there is no quick way to observe past performance other than to checkout, build, and run the performance tests for the commits in question.
* **Testing locally is not optimal.** Again, the expected values are likely not targeted for the programmer's local platform leading to false positives and/or negatives when testing locally.

== Proposed System

* Tests are listed in `.T` files along with manually chosen tolerance percentage (but no expected value and never platform specific, that is handled automatically). e.g. testing 'bytes allocated' with a tolerance of 10%:
{{{
test('T5631',
  [
    collect_compiler_stats('bytes allocated', 10),
    only_ways(['normal'])
  ],
  compile,
  [''])
}}}
* When running performance tests:
  * The results are compared to the previous commit's results (stored in a git note) with respective tolerances (stored in the `.T` files).
  * Tests fail if they are outside the tolerance percentage of the previous commit's result, unless the programmer has indicated in the current commit message that a change is expected (see [#ExpectedPerformanceChanges below]).
    * If the same test was run multiple times on the previous commit, then the average of those runs is used.
  * Tests trivially pass with a warning if results for the previous commit do not exist.
  * All test results (tagged with the current platform) are appended to the [https://git-scm.com/docs/git-notes git note] for the current commit.
* The CI server will automatically record performance results for each merged commit, pushing the performance results to the git.haskell.com repo.
* A simple CLI tool is provided along side the test driver to analyse previous results. E.g. comparing results for test `T123` between commits `$A`, `$B`, and `HEAD` can be done with the following command:
{{{
> python3 ./testsuite/driver/perf_notes.py --test-name T123 HEAD $B $A
}}}

=== Benefits

With the proposed change, we hope to achieve the following benefits:

* [#Platform Per platform] performance results means less variance of results. This allows for lower tolerance values and less false negatives without increasing false positives.
* Logged performance results makes analysing performance changes faster and easier.
* Less work for the programmer.
  * Expected values are no longer needed.
  * Expected performance changes only requires an [#ExpectedPerformanceChanges indication] that the change exists, rather than having to re-adjust the expected values and/or tolerance.
* More accurate [#TestingLocally local testing].

[=#Platform]
=== Test results are per platform

Performance results can vary between platforms. For example a 64 bit ghc may use more memory than a 32 bit ghc. Different OSes may also affect performance differently. Hence results are always recorded and compared with respect to a given platform. This is a key advantage over the current system. Performance results within a platform have less variance which allows for smaller tolerance values.

In the implementation of this proposal the platform is a string set by the TEST_ENV environment variable. This is set by all relevant CircleCI jobs (to e.g. 'x86_64-linux' or 'x86_64-freebsd') but defaults to 'local' (appropriate for running tests on your local machine).

[=#ExpectedPerformanceChanges]

=== Indicating expected performance changes

In some cases, performance tests will fail due to an expected change in performance. In this case, the programmer should double check that the change truly is expected. If the change truly is expected, they must append the git commit message to indicate this. For example, this indicates that the 'bytes allocated' metric increases for tests T123 and T456 and decreases for T789.
{{{
Metric Increase 'bytes allocated':
    T123
    T456
Metric Decrease 'bytes allocated':
    T789
}}}
The commit message will be parsed by the test driver and will allow the indicated changes. The exact text required to pass all failing tests will conveniently be output by the test driver if any performance tests fail.

[=#TestingLocally]
=== Testing Locally

Often the programmer wants to run performance tests on their local machine. To make a valid comparison of performance, we require results obtained on the same machine (i.e. the same platform). To do this, simply check out the previous commit and run the tests. This will record the results in a git note for the 'local' platform (this git note is stored in your local git repo). Then checkout the original commit and run the tests again. Now the test runner will test against the locally obtained results of the previous commit. If you don't run the tests on the previous commit, then they will trivially pass with a warning.

== Status

Most of the work on this was already done by Jared Weakly for the 2017 Haskell Summer of Code ([https://github.com/jared-w/HSOC2017/blob/master/Proposal.pdf HSOC proposal], [https://jaredweakly.com/blog/haskell-summer-of-code/ blog]). A [https://phabricator.haskell.org/D3758 work in progress patch] was submitted to Phabricator that summer. Unfortunately HSOC ended without a merge, and development on this proposal stalled. Since then David Eichmann has continued the work and created a [https://phabricator.haskell.org/D5059 new patch], an adapted (and rebased) version of the old patch. 

=== Finished Work
* Nolonger specify expected performance results in `*.T` files.
* Performance tests run against the previous commit's results.
* Log performance results in a git note for all test runs.
* A simple CLI tool is provided to analyse previous results.
* Recording and comparing performance results is always done with respect to a platform.

=== Remaining Work

* CircleCI must push test restults (git notes) to the git.haskell.com repo.
* Hear the concerns of the Haskell community.

== Future work

* With performance results saved as git notes, we are open to creating better tool support for analysing that data.
* Though a large part of the benefit of this change would be to lower tolerance percentages for tests, that will only be done later, after results have been collected.
* The change to comparing with the previous commit means that performance results may drift in one direction without tests ever failing. We could benefit from some automatic means of detecting such drift.

== See also

* Issue #12758.