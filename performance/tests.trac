== This page is a work in progress.

= Proposed Change to GHC Performance Tests

GHC has a test suite that can be run via `make test` (see [https://ghc.haskell.org/trac/ghc/wiki/Building/RunningTests/Running Running the Testsuite]). A subset of the tests are performance tests, which either test the performance of code generated by GHC or test the performance of GHC itself. This proposal is concerned with such performance tests and is motivated by various shortcomings in the current system.

The fundamental change proposed here is for the programmer to no longer specify expected values for performance tests. Instead, with every test run, actual values will be recorded automatically in a [https://git-scm.com/docs/git-notes git note].

TODO super simple intro
* we have a test driver (some links to other wiki pages)

TODO For the 2017 Haskell Summer of Code, Jared Weekly largely implemented the changes described in this page ([https://github.com/jared-w/HSOC2017/blob/master/Proposal.pdf HSOC proposal], [https://jaredweakly.com/blog/haskell-summer-of-code/ blog], stemming from issue #12758.

TODO Why was it delayed?

TODO Explain exactly what you mean by "Platform" (i.e. os, build settings etc.)

== Goals of Performance Testing

TODO do we need this section?
1. Automatically (via CI) track performance of compiler/generated code with every commit.
2. Automatically (via CI) Inform the programmer of performance regressions.
3. Allow the programmer to test locally.
4. ...?

== Motivation

This change is motivated by some shortcomings in the current system regarding performance tests:

TODO this is too wordy!

* **The programmer must specify expected values and tolerances.** This seems like a reasonable expectation but is a source of frustration in practice. The CI server will run tests on various platforms which may all give varying results, making it hard for the programmer to pick an appropriate value. This leads to a time consuming workflow of pushing code to the CI server and observing its output to derive expected values.
* **The programmer must update expected values.** When performance changes significantly, such that performance tests fail, the programmer must pick new expected values and/or adjust the tolerance values. Picking these values again is more effort than we'd like.
* **Expected values are usually NOT per platform.** While it is currently possible to give expected values per platform, doing so takes extra effort for the programmer and is often not done. Instead the programmer often picks some middle value with a tolerance that works for all platforms.
* **Tolerance intervals are too large.** As the expected values are usually not platform specific, the tolerance values must be larger to make the tests pass in all cases (avoiding false positives). This leads to more false negatives (tests pass even though performance has changed unexpectedly).
* **Performance test results are not logged.** Currently there is no quick way to observe past performance other than to checkout, build, and run the performance tests for the commits in question.
* **Testing locally is not optimal.** Again, the expected values are likely not targeted for the programmer's local platform leading to false positives and/or negatives when testing locally.

== Current System

* Tests are listed in `.T` files along with manually chosen expected value and tolerance percentage. Expected values may be, but often aren't, platform specific.
* When running performance tests:
  * The results are compared to the expected values with respective tolerances.
  * Tests fail if they are outside the tolerance percentage from the expected value.

== Proposed System

* Tests are listed in `.T` files along with manually chosen tolerance percentage (but no expected value).
* Performance results are stored per platform in [https://git-scm.com/docs/git-notes git notes] attached to the git commit they were run on.
* When running performance tests:
  * The results are compared to the previous commit's results (stored in a git note) with respective tolerances (stored in the `.T` files).
  * Tests fail if they are outside the tolerance percentage from the previous commit's result, unless the programmer has indicated in the current commit message that a change is expected.
  * All test results (tagged with the current platform) are appended to the git note for the current commit.
* When a patch is merged, CircleCI will:
  * Run the performance tests.
  * Push the new performance results (the git note) to the git.haskell.com repo.
* A simple CLI tool is provided to inspect performance results stored in git notes.

== What

* CI record results in git notes
* Results are recorded per platform
* No metrics in *.T files. Compare with previous commit

* Git note recorded after merging commit
* Always test against previous commit

== Implementation

TODO
* Tests are run via python script
* git notes

== Status

=== Done

=== Remaining Work

== Future work

* Decrease thresholds

== Open Issues

* Thresholding
* Local testing (no git note for previous commit)
* Drift (better option that compare with last)