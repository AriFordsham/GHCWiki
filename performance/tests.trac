== This page is a work in progress.

TODO wording: results vs metrics vs values

= Proposed Change to GHC Performance Tests

GHC has a test suite that can be run via `make test` (see [https://ghc.haskell.org/trac/ghc/wiki/Building/RunningTests/Running Running the Testsuite]). A subset of the tests are performance tests, which either test the performance of code generated by GHC or test the performance of GHC itself. This proposal is concerned with such performance tests and is motivated by various shortcomings in the current system.

The fundamental changes proposed here are:
* Don't specify expected performance results in `*.T` files.
  * Performance tests run against the previous commit's results instead of the expected results.
* Log performance results in a [https://git-scm.com/docs/git-notes git note] for all test runs.
  * The CI server will automatically record performance results for each merged commit, pushing the performance results to the git.haskell.com repo.
  * A simple CLI tool is provided to analyse previous results.
* Performance results are always recorded and compared with respect to a [#Platform platform].

TODO super simple intro
* we have a test driver (some links to other wiki pages)

TODO For the 2017 Haskell Summer of Code, Jared Weekly largely implemented the changes described in this page ([https://github.com/jared-w/HSOC2017/blob/master/Proposal.pdf HSOC proposal], [https://jaredweakly.com/blog/haskell-summer-of-code/ blog], stemming from issue #12758.

TODO Why was it delayed?

== Motivation

This proposed change is motivated by some shortcomings in the [#CurrentSystem current system] regarding performance tests:

[=#CurrentSystem]
== Current System

* Tests are listed in `.T` files along with manually chosen expected value and tolerance percentage. Expected values may be, but often aren't, platform specific. e.g.  testing 'bytes allocated' with a tolerance of 10%:
{{{
test('T5631',
  [
    compiler_stats_num_field(
      'bytes allocated',
      [
        (wordsize(32), 570137436, 10),
        (wordsize(64), 1106015512, 5)
      ]),
    only_ways(['normal'])
  ],
  compile,
  [''])
}}}
* When running performance tests:
  * The results are compared to the expected values with respective tolerances.
  * Tests fail if they are outside the tolerance percentage from the expected value.

=== Issues with the current system

* **The programmer must specify expected values and tolerances.** This seems like a reasonable expectation but is a source of frustration in practice. The CI server will run tests on various platforms which may all give varying results, making it hard for the programmer to pick an appropriate value. This leads to a time consuming workflow of pushing code to the CI server and observing its output to derive expected values. This leads to other issues:
  * **The programmer must update expected values.** When performance significantly changes, such that performance tests fail, the programmer must pick new expected values and/or adjust the tolerance values. Picking these values again is more effort than we'd like.
  * **Expected values are usually NOT per platform.** While it is currently possible to give expected values per platform, doing so takes extra effort for the programmer and is often not done. Instead the programmer often picks some middle value with a tolerance that works for all platforms.
  * **Tolerance intervals are too large.** As the expected values are usually not platform specific, the tolerance values must be larger to make the tests pass in all cases (avoiding false positives). This leads to more false negatives (tests pass even though performance has changed unexpectedly).
* **Performance test results are not logged.** Currently there is no quick way to observe past performance other than to checkout, build, and run the performance tests for the commits in question.
* **Testing locally is not optimal.** Again, the expected values are likely not targeted for the programmer's local platform leading to false positives and/or negatives when testing locally.

== Proposed System

* Tests are listed in `.T` files along with manually chosen tolerance percentage (but no expected value and never platform specific). e.g. testing 'bytes allocated' with a tolerance of 10%:
{{{
test('T5631',
  [
    collect_compiler_stats('bytes allocated', 10),
    only_ways(['normal'])
  ],
  compile,
  [''])
}}}
* Performance results are automatically stored per platform in [https://git-scm.com/docs/git-notes git notes] attached to the git commit they were run on.
* When running performance tests:
  * The results are compared to the previous commit's results (stored in a git note) with respective tolerances (stored in the `.T` files).
  * Tests fail if they are outside the tolerance percentage of the previous commit's result, unless the programmer has indicated in the current commit message that a change is expected (see [#ExpectedPerformanceChanges below]).
  * Tests trivially pass with a warning if results for the previous commit do not exist.
  * All test results (tagged with the current platform) are appended to the git note for the current commit.
* The CI server will automatically record performance results for each merged commit, pushing the performance results to the git.haskell.com repo.
* A simple CLI tool is provided to inspect performance results stored in git notes.

=== Benefits

With the proposed change, we hope to achieve the following benefits:

* [#Platform Per platform] performance results means less variance of results. This allows for lower tolerance values and less false negatives without increasing false positives.
* Logged performance results makes analysing performance changes easier.
* Less work for the programmer.
  * Expected values are no longer needed.
  * Expected performance changes only requires an [#ExpectedPerformanceChanges indication] that the change exists, rather than having to re-adjust the expected values and/or tolerance.
* More accurate [#TestingLocally local testing].

[=#ExpectedPerformanceChanges]
=== Indicating expected performance changes

In some cases, performance tests will fail due to an expected change in performance. In this case, the programmer should double check that the change truly is expected. If not, then they should fix the issue. Otherwise, they must append the git commit message to indicate the expected to change. For example, this indicates that the 'bytes allocated' metric increases for tests T123 and T456.
{{{
Metric Increase 'bytes allocated':
    T123
    T456
}}}
The commit message will be parsed by the test driver and will allow the indicated changes. The exact text required to pass all failing tests will conveniently be output by the test driver if any performance tests fail.

[=#TestingLocally]
=== Testing Locally

Often the programmer wants to run performance tests on their local machine. We want to use results obtained on the same machine (i.e. the same platform). To do this, simply check out the previous commit and run the tests. This will record the results in a git note for the 'local' platform (this git note is stored in your local git repo). Then checkout the next commit and run the tests again. Now the test runner will compare the locally obtained results from the previous commit. If you haven't run a test on the previous commit, then it will trivially pass with a warning.

[=#Platform]
=== Test results are per platform

Performance results can vary between platforms. For example a 64 bit ghc may use more memory than a 32 bit ghc. Different OSes may also affect performance differently. Hence results are always recorded and compared with respect to a given platform. This is a key advantage over the current system. Performance results within a platform have less variance which allows for smaller tolerance values.

In the implementation of this proposal the platform is a string set by the TEST_ENV environment variable. This is set by all relevant CircleCI jobs (to e.g. 'x86_64-linux' or 'x86_64-freebsd') and defaults to 'local' (appropriate for running test on your local machine).

== Status

=== Done

=== Remaining Work

== Future work

* Decrease thresholds

== Open Issues

* Thresholding
* Local testing (no git note for previous commit)
* Drift (better option that compare with last)