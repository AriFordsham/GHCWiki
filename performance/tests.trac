== This page is a work in progress.

= Proposed Change to GHC Performance Tests

GHC has a test suite that can be run via `make test` (see [https://ghc.haskell.org/trac/ghc/wiki/Building/RunningTests/Running Running the Testsuite]). A subset of the tests in the test suite are performance tests, which either test the performance of code generated by GHC or test the performance of GHC itself. This proposal is concerned with such performance tests and is motivated by various shortcomings in the current system.

TODO super simple intro
* we have a test driver (some links to other wiki pages)

TODO For the 2017 Haskell Summer of Code, Jared Weekly largely implemented the changes described in this page ([https://github.com/jared-w/HSOC2017/blob/master/Proposal.pdf HSOC proposal], [https://jaredweakly.com/blog/haskell-summer-of-code/ blog], stemming from issue #12758.

TODO Why was it delayed?

== Goals of Performance Testing

TODO do we need this section?
1. Automatically (via CI) track performance of compiler/generated code with every commit.
2. Automatically (via CI) Inform the programmer of performance regressions.
3. Allow the programmer to test locally.
4. ...?

== Motivation

This change is motivated by some shortcomings in the current system regarding performance tests:

* **The programmer must specify expected values and tolerances.** This seems like a reasonable expectation but is a source of frustration in practice. The CI server will run tests on various platforms which may all give varying results, making it hard for the programmer to pick an appropriate value. This leads to a time consuming workflow of pushing code to the build server and observing its output to derive expected values.
* **The programmer must update expected values.** When performance changes significantly, such that performance tests fail, the programmer must pick new expected values and/or adjust the tolerance values. Picking these values again is more effort than we'd like.
* **Expected values are usually NOT per platform.** While it is currently possible to give expected values per platform, doing so takes extra effort for the programmer and is often not done. Instead the programmer often picks some middle value with a tolerance that works for all platforms.
* **Tolerance intervals are too large.** As the expected values are usually not platform specific, the tolerance values must be larger to make the tests pass in all cases.
* **Performance test results should be logged.** Currently there is no quick way to observe past performance other than to checkout, build, and run the performance tests for the commits in question.

== Current Workflow

=== Adding a test case
=== Creating and Submitting a Patch


== Proposed Workflow

== What

* CI record results in git notes
* Results are recorded per platform
* No metrics in *.T files. Compare with previous commit

* Git note recorded after merging commit
* Always test against previous commit

== Implementation

TODO
* Tests are run via python script
* git notes

== Status

=== Done

=== Remaining Work

== Future work

* Decrease thresholds

== Open Issues

* Thresholding
* Local testing (no git note for previous commit)
* Drift (better option that compare with last)